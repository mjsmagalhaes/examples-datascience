{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOyDV2jLDg10"
   },
   "source": [
    "# Restaurant Review Analysis\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/mjsmagalhaes/examples-datascience/blob/main/restaurant_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "A notebook investigating a [restaurant review dataset](https://www.kaggle.com/bulentsiyah/restaurant-reviews) from kaggle.\n",
    "\n",
    "The solution here uses a Neural Network (NN) as a scoring system and a Decision Tree (DT, which is just making the solution worse :) -- but it is a fun little test casding classifiers) to \"decide\" if a score means a good review or a bad one.\n",
    "\n",
    "Neither the NN nor the DT is doing a good job :).\n",
    "\n",
    "**This is a work in progress.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJVm71G-D9J7"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tPfSFWfC-lMg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_vocab' from 'dslib.nlp' (c:\\Users\\PICHAU\\Desktop\\Arquivos\\examples-datascience\\dslib\\nlp.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15232/1881522046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdslib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_from_csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdslib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'extract_vocab' from 'dslib.nlp' (c:\\Users\\PICHAU\\Desktop\\Arquivos\\examples-datascience\\dslib\\nlp.py)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import altair as alt\n",
    "import graphviz as graph\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text, plot_tree\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from dslib import pandas_from_csv\n",
    "from dslib.nlp import extract_vocab, vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC-XBjhYDtOZ"
   },
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bZH4eh3_KUj"
   },
   "outputs": [],
   "source": [
    "raw = pandas_from_csv('data/Restaurant_Reviews.csv')\n",
    "corpus = raw['Review'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiyFapl4EQR7"
   },
   "source": [
    "## Create Sets: Train, Validation and TestCreate Sets: Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUzClQLCAZmy"
   },
   "outputs": [],
   "source": [
    "# Train: 60%; Valid: 20%; Test: 20%\n",
    "xTV, testCorpus, yTV, testTarget = train_test_split(\n",
    "    raw['Review'], \n",
    "    raw['Liked'], \n",
    "    test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "trainCorpus, validCorpus, trainTarget, validTarget = train_test_split(\n",
    "    xTV, yTV, \n",
    "    test_size=0.25, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find \"meaningful\" words in the corpus\n",
    "vocabs = extract_vocab(trainCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform corpus sentences into one-hot vectors\n",
    "trainX = pd.DataFrame(vectorize(trainCorpus, vocabs), columns=vocabs)\n",
    "validX = pd.DataFrame(vectorize(validCorpus, vocabs), columns=vocabs)\n",
    "testX = pd.DataFrame(vectorize(testCorpus, vocabs), columns=vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape, trainTarget.shape, len(vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL87Jp2dEZtc"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_JRAOViB0gI"
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.Orthogonal()\n",
    "constraint = tf.keras.constraints.NonNeg()\n",
    "\n",
    "n_dim = len(vocabs)\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(\n",
    "        units=math.ceil(n_dim/2), activation='tanh', input_shape=(n_dim,),\n",
    "        kernel_initializer=initializer, \n",
    "        # kernel_constraint=constraint,\n",
    "        # kernel_regularizer='l2'\n",
    "    ),\n",
    "     Dense(\n",
    "        units=math.ceil(n_dim/4), activation='tanh', \n",
    "        kernel_initializer=initializer, \n",
    "        # kernel_constraint=constraint, \n",
    "        # kernel_regularizer='l2'\n",
    "    ),\n",
    "    Dense(\n",
    "        units=math.ceil(n_dim/8), activation='tanh', \n",
    "        kernel_initializer=initializer, \n",
    "        # kernel_constraint=constraint, \n",
    "        # kernel_regularizer='l2'\n",
    "    ),\n",
    "    Dense(\n",
    "        units=1,activation='tanh'\n",
    "    )\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(momentum=0.4,learning_rate=0.025),\n",
    "    loss=\"mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "KK4VTSUkCNQo",
    "outputId": "102074dd-1ff1-4086-e075-6f6e54a205e9"
   },
   "outputs": [],
   "source": [
    "th = model.fit(\n",
    "    trainX,\n",
    "    trainTarget,\n",
    "    epochs=300,\n",
    "    # steps_per_epoch=1,\n",
    "    # validation_split=None, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pd.DataFrame(th.history.get('loss'), columns=['loss']).plot.line()\n",
    "validPred = model.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-uQAFigCWwn"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4).fit(validPred, validTarget)\n",
    "# Save Tree\n",
    "g = graph.Source(export_graphviz(tree))\n",
    "# g\n",
    "# g.render(filename='tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCzbRinUFGO-"
   },
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "OdDpmvAMCwTA",
    "outputId": "4ea802d5-9acd-4648-a04c-e021b8b523ba"
   },
   "outputs": [],
   "source": [
    "# Neural Network Score\n",
    "testPred = model.predict(testX)\n",
    "\n",
    "# Plot Neural Network Score Distribution\n",
    "z = pd.DataFrame(testPred, columns=['Score'])\n",
    "z = z.join(testTarget.reset_index())\n",
    "\n",
    "sns.displot(z, x='Score', hue='Liked', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1 = alt.Chart(z).transform_density(\n",
    "    density='Score',\n",
    "    # bandwidth=0.2,\n",
    "    groupby=['Liked'],\n",
    "    extent= [-1, 1],\n",
    "    counts = True,\n",
    "    steps=200,\n",
    "    as_ = [\"Score\", \"Density\"]\n",
    ").mark_area(line=True, opacity=0.6).encode(\n",
    "    alt.X('Score:Q'),\n",
    "    alt.Y('Density:Q', stack=None),\n",
    "    alt.Color('Liked:N')\n",
    ").interactive()\n",
    "\n",
    "chart_2 = alt.Chart(z).mark_bar().encode(\n",
    "    alt.X('Score', bin=alt.BinParams(step=0.05, extent=[-1, 1])),\n",
    "    alt.Y('count()', stack='normalize'),\n",
    "    color='Liked:N',\n",
    "    tooltip='count()'\n",
    ").interactive()\n",
    "\n",
    "chart_1 | chart_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both density estimation figures above they extend beyond the [-1,1] range of the hyperbolic tangent that is the activation function of the output layer. The Neural Network is having a hard time finding a implementing a function that can provide good separability for those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networking \"Post-Processing\"\n",
    "testPred_Tree = tree.predict(testPred)\n",
    "testPred_NN = list(map(lambda y: 0 if y < 0.55 else 1, testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEDkR4amE6El"
   },
   "outputs": [],
   "source": [
    "a.evaluate(testTarget, testPred_NN, title='Neural Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.evaluate(testTarget, testPred_Tree, title='NN > Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.weights[0][:,1], columns=['W']).plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(1090)\n",
    "\n",
    "source = pd.DataFrame({\n",
    "  'x': x,\n",
    "  'y': model.weights[0][:,0].numpy()\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line().encode(x='x',y='y').interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDgmxV6Q7+iorgRycH7sDu",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "restaurant_reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
